import requests
import re
from datetime import datetime
import os
from urllib.parse import urlparse

def extract_stream_urls_from_page(page_url):
    """Streamtest.in sayfasÄ±ndan stream URL'lerini Ã§Ä±kar (DÃœZELTÄ°LDÄ°)"""
    try:
        print(f"ğŸ“„ Fetching: {page_url}")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        
        response = requests.get(page_url, headers=headers, timeout=15)
        
        if response.status_code != 200:
            print(f"  âŒ HTTP {response.status_code}")
            return []
        
        print(f"  âœ… Status: {response.status_code}, Length: {len(response.text)} chars")
        
        # DEBUG: Ä°lk 1000 karakteri kaydet
        with open("debug_page.html", "w", encoding="utf-8") as f:
            f.write(response.text[:2000])
        
        stream_urls = []
        lines = response.text.split('\n')
        
        # Ã–NEMLÄ°: Streamtest.in sayfa yapÄ±sÄ± analizi
        # URL'ler genellikle ÅŸu formatta:
        # 1. Zaman bilgisi olan satÄ±r
        # 2. BoÅŸ satÄ±r  
        # 3. URL satÄ±rÄ±
        # 4. "| Detail" satÄ±rÄ±
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            # URL iÃ§eren satÄ±rlarÄ± bul
            if line.startswith('http://') or line.startswith('https://'):
                # "Detail" sayfasÄ± linki mi kontrol et
                if 'streamtest.in/logs/' in line and 'Detail' in lines[i+1] if i+1 < len(lines) else False:
                    continue  # Bu "Detail" linki, atla
                
                # Bu bir stream URL'si mi?
                if is_likely_stream_url(line):
                    # URL'yi temizle
                    clean_url = line.split(' ')[0].split('|')[0].strip()
                    
                    # GeÃ§ersiz karakterleri kontrol et
                    if ' ' in clean_url or '<' in clean_url:
                        continue
                    
                    # "Detail" kelimesini iÃ§eriyor mu?
                    if 'Detail' in clean_url:
                        continue
                    
                    # Benzersizse ekle
                    if clean_url not in stream_urls:
                        stream_urls.append(clean_url)
                        print(f"    â• Found: {clean_url[:80]}...")
        
        print(f"  ğŸ“Š Found {len(stream_urls)} URLs on this page")
        return stream_urls
        
    except Exception as e:
        print(f"  âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return []

def is_likely_stream_url(url):
    """URL'nin stream URL'si olma olasÄ±lÄ±ÄŸÄ±nÄ± kontrol et"""
    
    # KESÄ°NLÄ°KLE ATLANACAK pattern'ler
    exclude_patterns = [
        'streamtest.in',      # Kendi sayfasÄ±
        '/detail',            # Detail sayfalarÄ±
        'favicon',            # Favicon
        '.css', '.js',        # CSS/JS dosyalarÄ±
        '.png', '.jpg', '.gif', '.ico',  # Resimler
    ]
    
    for pattern in exclude_patterns:
        if pattern in url.lower():
            return False
    
    # STREAM OLMA Ä°HTÄ°MALÄ° YÃœKSEK pattern'ler
    stream_patterns = [
        '.m3u', '.m3u8', '.mpd',                    # Stream formatlarÄ±
        'get.php',                                   # IPTV get.php
        '.php?id=', '.php?type=', '.php?auth=',     # PHP stream script'leri
        '/live/', '/stream/', '/playlist/',         # Stream yollarÄ±
        '/manifest.', '/chunklist.',                # HLS/DASH manifest
        ':8080/', ':1935/', ':80/',                 # Stream portlarÄ±
        '/hls/', '/dash/', '/mpegts/',              # Stream path'leri
    ]
    
    for pattern in stream_patterns:
        if pattern in url.lower():
            return True
    
    # Alternatif: Domain analizi
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        
        # Bilinen stream domain pattern'leri
        stream_domains = ['.xyz', '.top', '.cn', '.cloud', '.tv', '.net', 'akamaized']
        for d in stream_domains:
            if d in domain:
                return True
    except:
        pass
    
    return False

def collect_all_streams():
    """TÃ¼m sayfalardan stream'leri topla"""
    all_streams = []
    base_url = "https://streamtest.in/logs/page/"
    
    # TEST: Sadece ilk 2 sayfa ile baÅŸla
    max_pages = 2
    
    print(f"\nğŸ” Scanning {max_pages} pages from {base_url}")
    print("=" * 60)
    
    for page_num in range(1, max_pages + 1):
        page_url = f"{base_url}{page_num}"
        page_streams = extract_stream_urls_from_page(page_url)
        
        if page_streams:
            all_streams.extend(page_streams)
        else:
            print(f"  âš ï¸ No URLs found on page {page_num}, stopping scan")
            break
        
        # KÄ±sa bekle (rate limiting iÃ§in)
        import time
        time.sleep(1)
    
    # Tekilleri al
    unique_streams = list(set(all_streams))
    unique_streams.sort()
    
    print(f"\nğŸ“Š TOTAL: Found {len(unique_streams)} unique streams")
    
    # DEBUG: BulunanlarÄ± kaydet
    with open("found_urls.txt", "w", encoding="utf-8") as f:
        for url in unique_streams:
            f.write(url + "\n")
    
    return unique_streams

def create_m3u_file(stream_urls, filename="streams.m3u"):
    """M3U dosyasÄ± oluÅŸtur"""
    if not stream_urls:
        print("âŒ No streams to write to M3U file!")
        
        # BoÅŸ da olsa M3U dosyasÄ± oluÅŸtur (header ile)
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("#EXTM3U\n")
            f.write(f"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("# WARNING: No streams found!\n")
            f.write("# Check the extraction logic.\n")
        
        print(f"âš ï¸ Created empty M3U file: {filename}")
        return
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("#EXTM3U\n")
        f.write(f"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"# Total streams: {len(stream_urls)}\n")
        f.write("# Source: streamtest.in\n")
        f.write("# Format: #EXTINF:-1, Channel Name\n")
        f.write("#         URL\n\n")
        
        for i, url in enumerate(stream_urls, 1):
            # Kanal adÄ±nÄ± oluÅŸtur
            try:
                parsed = urlparse(url)
                domain = parsed.netloc
                path = parsed.path
                
                # Path'ten isim Ã§Ä±kar
                if path:
                    # Son segmenti al
                    name = path.split('/')[-1]
                    if '.' in name:
                        name = name.split('.')[0]
                    
                    if name and len(name) > 1:
                        channel_name = f"Channel_{i}_{name}"
                    else:
                        channel_name = f"Channel_{i}_{domain}"
                else:
                    channel_name = f"Channel_{i}_{domain}"
                    
                # Ã‡ok uzunsa kÄ±salt
                if len(channel_name) > 50:
                    channel_name = channel_name[:47] + "..."
                    
            except:
                channel_name = f"Stream_{i}"
            
            # M3U formatÄ±nda yaz
            f.write(f"#EXTINF:-1, {channel_name}\n")
            f.write(f"{url}\n")
    
    print(f"âœ… M3U file created: {filename}")
    print(f"ğŸ“ File size: {os.path.getsize(filename)} bytes")
    
    # Ä°lk birkaÃ§ satÄ±rÄ± gÃ¶ster
    print("\nğŸ“„ First 5 entries:")
    with open(filename, 'r', encoding='utf-8') as f:
        lines = f.readlines()[:15]
        for line in lines:
            print(f"  {line.rstrip()}")

def main():
    print("ğŸš€ Stream Collector - DEBUG VERSION")
    print("=" * 60)
    
    # Debug bilgileri
    print("Python version:", os.sys.version)
    print("Current dir:", os.getcwd())
    print("Files in dir:", os.listdir('.'))
    
    # TÃ¼m stream'leri topla
    all_streams = collect_all_streams()
    
    # M3U dosyasÄ± oluÅŸtur
    create_m3u_file(all_streams)
    
    # EÄŸer hala boÅŸsa, test yap
    if not all_streams:
        print("\nğŸ”§ DEBUG MODE: Testing extraction manually...")
        test_extraction()

def test_extraction():
    """Manuel test fonksiyonu"""
    test_url = "https://streamtest.in/logs/page/1"
    
    print(f"\nğŸ§ª Testing extraction from: {test_url}")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(test_url, headers=headers, timeout=10)
        
        print(f"Status: {response.status_code}")
        print(f"Length: {len(response.text)} characters")
        
        # TÃ¼m http/https baÄŸlantÄ±larÄ±nÄ± bul
        all_urls = re.findall(r'https?://[^\s<>"\']+', response.text)
        
        print(f"\nFound {len(all_urls)} total URLs in page")
        
        # Ä°lk 10 URL'yi gÃ¶ster
        print("\nFirst 10 URLs found:")
        for i, url in enumerate(all_urls[:10], 1):
            print(f"{i:2}. {url}")
            
        # Stream olabilecekleri filtrele
        stream_urls = [url for url in all_urls if is_likely_stream_url(url)]
        
        print(f"\nFiltered to {len(stream_urls)} likely stream URLs:")
        for i, url in enumerate(stream_urls[:10], 1):
            print(f"{i:2}. {url}")
            
        # Debug iÃ§in kaydet
        with open("test_all_urls.txt", "w", encoding="utf-8") as f:
            for url in all_urls:
                f.write(url + "\n")
                
        with open("test_stream_urls.txt", "w", encoding="utf-8") as f:
            for url in stream_urls:
                f.write(url + "\n")
                
    except Exception as e:
        print(f"Test error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
