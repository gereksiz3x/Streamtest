import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import os

def extract_stream_urls_from_page(page_url):
    """Tek bir sayfadan stream URL'lerini Ã§Ä±kar"""
    try:
        print(f"ğŸ“„ Fetching: {page_url}")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(page_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Sayfadaki tÃ¼m baÄŸlantÄ±larÄ± bul
        stream_urls = []
        
        # 1. DoÄŸrudan <a> tag'lerindeki href'leri kontrol et
        for link in soup.find_all('a', href=True):
            href = link['href']
            if is_stream_url(href):
                stream_urls.append(href)
        
        # 2. Sayfa iÃ§indeki dÃ¼z metinde URL'leri ara (gÃ¼Ã§lÃ¼ regex)
        text_urls = re.findall(r'https?://[^\s<>"\']+', response.text)
        for url in text_urls:
            if is_stream_url(url) and url not in stream_urls:
                stream_urls.append(url)
        
        # 3. Sayfadaki dÃ¼z metin satÄ±rlarÄ±nda URL ara
        lines = response.text.split('\n')
        for line in lines:
            # "http" ile baÅŸlayan ve "Detail" iÃ§ermeyen satÄ±rlar
            if 'http' in line and 'Detail' not in line:
                # SatÄ±rdaki URL'yi Ã§Ä±kar
                url_match = re.search(r'(https?://[^\s<>"\']+)', line)
                if url_match:
                    url = url_match.group(1)
                    if is_stream_url(url) and url not in stream_urls:
                        stream_urls.append(url)
        
        print(f"  âœ… Found {len(stream_urls)} URLs")
        return list(set(stream_urls))  # Tekilleri dÃ¶ndÃ¼r
        
    except Exception as e:
        print(f"  âŒ Error fetching {page_url}: {e}")
        return []

def is_stream_url(url):
    """URL'nin stream URL'si olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
    # Ä°stenmeyen URL'leri filtrele
    if 'detail' in url.lower() or 'streamtest.in' in url:
        return False
    
    # Stream dosya uzantÄ±larÄ±/pattern'leri
    stream_patterns = [
        '.m3u', '.m3u8', '.mpd',          # Stream formatlarÄ±
        'get.php?type=m3u',                # IPTV get.php
        '/playlist', '/live/', '/stream',  # Stream yollarÄ±
        'manifest.', 'chunklist.',         # HLS/DASH
        'id=tvb', 'id=sz', 'id=%'          # Sayfadaki Ã¶zel pattern'ler
    ]
    
    # URL stream pattern'lerinden birini iÃ§eriyor mu?
    for pattern in stream_patterns:
        if pattern in url.lower():
            return True
    
    return False

def collect_all_streams():
    """TÃ¼m sayfalardan stream'leri topla"""
    all_streams = []
    base_url = "https://streamtest.in/logs/page/"
    
    # KaÃ§ sayfa tarayacaÄŸÄ±mÄ±zÄ± belirle
    max_pages = 5  # Ä°stediÄŸiniz sayfa sayÄ±sÄ±
    
    for page_num in range(1, max_pages + 1):
        page_url = f"{base_url}{page_num}"
        page_streams = extract_stream_urls_from_page(page_url)
        all_streams.extend(page_streams)
    
    # Tekilleri al ve sÄ±rala
    unique_streams = list(set(all_streams))
    unique_streams.sort()
    
    print(f"\nğŸ“Š Total unique streams found: {len(unique_streams)}")
    return unique_streams

def create_m3u_file(stream_urls, filename="streams.m3u"):
    """M3U dosyasÄ± oluÅŸtur"""
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("#EXTM3U\n")
        f.write(f"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"# Total streams: {len(stream_urls)}\n")
        f.write("# Source: streamtest.in\n\n")
        
        for i, url in enumerate(stream_urls, 1):
            # Kanal ismini URL'den Ã§Ä±kar
            channel_name = f"Stream_{i}"
            
            # URL'den anlamlÄ± isim Ã§Ä±karmaya Ã§alÄ±ÅŸ
            try:
                domain = url.split('/')[2] if len(url.split('/')) > 2 else "Unknown"
                # Ã–zel durumlar
                if 'get.php' in url:
                    if 'username=' in url:
                        channel_name = f"IPTV_{domain}"
                    else:
                        channel_name = f"IPTV_Stream_{i}"
                elif '.m3u8' in url:
                    channel_name = f"HLS_Stream_{domain}"
                elif '.mpd' in url:
                    channel_name = f"DASH_Stream_{domain}"
                elif 'id=' in url or 'id=%' in url:
                    channel_name = f"TV_Channel_{i}"
                else:
                    channel_name = f"Stream_{domain}"
            except:
                channel_name = f"Stream_{i}"
            
            # M3U formatÄ±nda yaz
            f.write(f"#EXTINF:-1, {channel_name}\n")
            f.write(f"{url}\n\n")
    
    print(f"âœ… M3U file created: {filename}")
    print(f"ğŸ“ File size: {os.path.getsize(filename)} bytes")

def main():
    print("ğŸš€ Starting Stream Collector...")
    print("=" * 50)
    
    # TÃ¼m stream'leri topla
    all_streams = collect_all_streams()
    
    if not all_streams:
        print("âŒ No streams found!")
        return
    
    # M3U dosyasÄ± oluÅŸtur
    create_m3u_file(all_streams)
    
    # Ä°statistikleri yazdÄ±r
    print("\nğŸ“ˆ Statistics:")
    print(f"   â€¢ Total URLs: {len(all_streams)}")
    
    # URL tÃ¼rlerini say
    m3u_count = sum(1 for url in all_streams if '.m3u' in url.lower())
    m3u8_count = sum(1 for url in all_streams if '.m3u8' in url.lower())
    mpd_count = sum(1 for url in all_streams if '.mpd' in url.lower())
    php_count = sum(1 for url in all_streams if 'get.php' in url.lower())
    
    print(f"   â€¢ M3U URLs: {m3u_count}")
    print(f"   â€¢ M3U8 URLs: {m3u8_count}")
    print(f"   â€¢ MPD URLs: {mpd_count}")
    print(f"   â€¢ get.php URLs: {php_count}")

if __name__ == "__main__":
    main()