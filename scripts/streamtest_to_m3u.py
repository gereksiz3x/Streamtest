#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
streamtest.in/logs sayfasÄ±ndaki son test edilmiÅŸ yayÄ±nlarÄ± Ã§ekip M3U playlist oluÅŸturur.
GitHub Actions ile otomatik Ã§alÄ±ÅŸacak ÅŸekilde tasarlanmÄ±ÅŸtÄ±r.
"""

import requests
import re
import os
import json
from datetime import datetime
from typing import List, Tuple, Optional, Dict
import sys

class StreamTestScraper:
    def __init__(self):
        self.base_url = "https://streamtest.in/logs"
        self.output_dir = "outputs"
        self.archive_file = os.path.join(self.output_dir, "stream_archive.json")
        
    def sayfayi_getir(self) -> Optional[str]:
        """SayfanÄ±n HTML iÃ§eriÄŸini getirir."""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(self.base_url, headers=headers, timeout=20)
            response.raise_for_status()
            return response.text
        except requests.exceptions.RequestException as e:
            print(f"âŒ Sayfa yÃ¼klenirken hata: {e}")
            return None

    def linkleri_ve_kanallari_bul(self, html_icerik: str) -> List[Dict]:
        """
        HTML iÃ§eriÄŸinden linkleri ve kanal bilgilerini Ã§Ä±karÄ±r.
        GeliÅŸmiÅŸ regex ve pattern tanÄ±ma kullanÄ±r.
        """
        bulunanlar = []
        
        # GeliÅŸmiÅŸ link patternleri
        link_patterns = [
            r'(https?://[^\s"\'<>]+?\.(?:m3u8|mp4|ts|mpd)[^\s"\'<>]*)',
            r'(https?://[^\s"\'<>]+/(?:live|playlist|master|stream|index)[^\s"\'<>]*\.m3u8[^\s"\'<>]*)',
            r'(https?://[^\s"\'<>]+/hls/[^\s"\'<>]*\.m3u8[^\s"\'<>]*)',
            r'(https?://[^\s"\'<>]+\.(?:akamaihd|cloudfront)\.net/[^\s"\'<>]*\.m3u8[^\s"\'<>]*)'
        ]
        
        # HTML'i satÄ±rlara bÃ¶l ve analiz et
        satirlar = html_icerik.split('\n')
        
        for i, satir in enumerate(satirlar):
            for pattern in link_patterns:
                link_match = re.search(pattern, satir, re.IGNORECASE)
                if link_match:
                    link = link_match.group(0).strip()
                    
                    # Kanal adÄ±nÄ± bul (geliÅŸmiÅŸ algoritma)
                    kanal_adi = self._kanal_adi_bul(satirlar, i, link)
                    
                    # Link tipini belirle
                    link_tipi = self._link_tipini_belirle(link)
                    
                    # Benzersizlik kontrolÃ¼
                    if not any(l['url'] == link for l in bulunanlar):
                        bulunanlar.append({
                            'url': link,
                            'kanal_adi': kanal_adi,
                            'tip': link_tipi,
                            'bulunma_zamani': datetime.now().isoformat(),
                            'kaynak_satir': i
                        })
                    break
        
        return bulunanlar

    def _kanal_adi_bul(self, satirlar: List[str], link_satiri_index: int, link: str) -> str:
        """En uygun kanal adÄ±nÄ± bulmak iÃ§in geliÅŸmiÅŸ algoritma"""
        kanal_adi = None
        
        # 1. Ã–nceki satÄ±rlarda isim ara
        for j in range(max(0, link_satiri_index-3), link_satiri_index):
            onceki_satir = satirlar[j].strip()
            if onceki_satir and len(onceki_satir) < 100 and not re.search(r'https?://', onceki_satir):
                # Tarih/saat ve gereksiz ifadeleri temizle
                temiz_metin = self._temizle_metin(onceki_satir)
                if temiz_metin and len(temiz_metin) > 2:
                    kanal_adi = temiz_metin
                    break
        
        # 2. Link iÃ§inden domain adÄ±nÄ± Ã§Ä±kar
        if not kanal_adi:
            domain_match = re.search(r'https?://([^/]+)', link)
            if domain_match:
                domain = domain_match.group(1)
                # Alt domain'leri temizle
                domain = re.sub(r'^www\.', '', domain)
                domain = domain.split('.')[0].capitalize()
                kanal_adi = domain
        
        # 3. HiÃ§bir ÅŸey bulunamazsa
        if not kanal_adi:
            kanal_adi = "Bilinmeyen Kanal"
        
        return kanal_adi

    def _temizle_metin(self, metin: str) -> str:
        """Metinden tarih, saat ve gereksiz ifadeleri temizler"""
        # Saat formatlarÄ±nÄ± temizle
        metin = re.sub(r'\d{1,2}:\d{2}\s*(?:AM|PM|am|pm)?', '', metin)
        metin = re.sub(r'\d{1,2}[/-]\d{1,2}[/-]\d{2,4}', '', metin)
        metin = re.sub(r'\d+\s*(?:minute|minutes|hour|hours|second|seconds)\s+ago', '', metin, flags=re.IGNORECASE)
        metin = re.sub(r'\|\s*Detail$', '', metin, flags=re.IGNORECASE)
        metin = re.sub(r'[<>"\'|]', '', metin)
        metin = re.sub(r'\s+', ' ', metin)
        
        return metin.strip()

    def _link_tipini_belirle(self, link: str) -> str:
        """Link tipini belirler"""
        if '.m3u8' in link.lower():
            return 'hls'
        elif '.mp4' in link.lower():
            return 'mp4'
        elif '.mpd' in link.lower():
            return 'dash'
        else:
            return 'unknown'

    def m3u_olustur(self, linkler: List[Dict], dosya_adi: str = None):
        """Bulunan linklerden M3U playlist oluÅŸturur"""
        if not dosya_adi:
            tarih = datetime.now().strftime('%Y%m%d_%H%M%S')
            dosya_adi = f"streamtest_{tarih}.m3u"
        
        # Ã‡Ä±ktÄ± klasÃ¶rÃ¼nÃ¼ oluÅŸtur
        os.makedirs(self.output_dir, exist_ok=True)
        dosya_yolu = os.path.join(self.output_dir, dosya_adi)
        
        with open(dosya_yolu, 'w', encoding='utf-8') as f:
            f.write("#EXTM3U\n")
            f.write(f"# {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - streamtest.in/logs'dan otomatik oluÅŸturuldu (GitHub Actions)\n")
            f.write(f"# Toplam {len(linkler)} yayÄ±n bulundu\n\n")
            
            for link in linkler:
                f.write(f'#EXTINF:-1 tvg-logo="" group-title="{link["tip"].upper()}",{link["kanal_adi"]}\n')
                f.write(f"{link['url']}\n\n")
        
        print(f"âœ… {len(linkler)} yayÄ±n '{dosya_yolu}' dosyasÄ±na kaydedildi")
        return dosya_yolu

    def arsivi_guncelle(self, yeni_linkler: List[Dict]):
        """Link arÅŸivini gÃ¼nceller"""
        eski_linkler = []
        
        # Eski arÅŸivi yÃ¼kle
        if os.path.exists(self.archive_file):
            try:
                with open(self.archive_file, 'r', encoding='utf-8') as f:
                    eski_linkler = json.load(f)
            except:
                pass
        
        # Yeni linkleri ekle
        tum_linkler = eski_linkler + yeni_linkler
        
        # URL bazlÄ± benzersiz yap (en son eklenen kalsÄ±n)
        unique = {}
        for link in tum_linkler:
            unique[link['url']] = link
        
        # Tekrar kaydet
        os.makedirs(self.output_dir, exist_ok=True)
        with open(self.archive_file, 'w', encoding='utf-8') as f:
            json.dump(list(unique.values()), f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“š ArÅŸiv gÃ¼ncellendi: {len(unique)} benzersiz link")

    def calistir(self):
        """Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu"""
        print("ğŸ” streamtest.in/logs taranÄ±yor...")
        
        html = self.sayfayi_getir()
        if not html:
            return False
        
        print("ğŸ“¡ Linkler ve kanal bilgileri Ã§Ä±karÄ±lÄ±yor...")
        linkler = self.linkleri_ve_kanallari_bul(html)
        
        if not linkler:
            print("âš ï¸ HiÃ§ link bulunamadÄ±!")
            return False
        
        print(f"âœ… {len(linkler)} link bulundu")
        
        # M3U oluÅŸtur
        m3u_dosyasi = self.m3u_olustur(linkler)
        
        # Ana M3U olarak da kaydet (son gÃ¼ncel)
        ana_m3u = os.path.join(self.output_dir, "son_yayinlar.m3u")
        with open(ana_m3u, 'w', encoding='utf-8') as f:
            with open(m3u_dosyasi, 'r', encoding='utf-8') as kaynak:
                f.write(kaynak.read())
        
        # ArÅŸivi gÃ¼ncelle
        self.arsivi_guncelle(linkler)
        
        return True

if __name__ == "__main__":
    scraper = StreamTestScraper()
    basarili = scraper.calistir()
    sys.exit(0 if basarili else 1)
